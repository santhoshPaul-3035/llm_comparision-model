# Self-contained Transformer implementation in PyTorch.
# Includes:
# - Scaled dot-product attention
# - Multi-head attention
# - Position-wise feed-forward
# - Positional encoding (sinusoidal)
# - EncoderLayer, DecoderLayer
# - Encoder, Decoder
# - Transformer (encoder-decoder)
# - Mask utilities
# - Example forward pass and small training loop skeleton

import math
from typing import Optional

import torch
import torch.nn as nn
import torch.nn.functional as F

# ---------------------------- Device ----------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# ---------------------------- Attention ----------------------------

def scaled_dot_product_attention(q, k, v, mask=None):
    """
    Calculate scaled dot-product attention.
    
    Args:
        q (Tensor): Queries. Shape (..., seq_len_q, d_k)
        k (Tensor): Keys. Shape (..., seq_len_k, d_k)
        v (Tensor): Values. Shape (..., seq_len_v, d_v) (seq_len_k == seq_len_v)
        mask (Tensor, optional): Mask tensor. Shape (..., seq_len_q, seq_len_k). Defaults to None.

    Returns:
        Tuple[Tensor, Tensor]: (Output tensor, Attention weights)
    """
    d_k = q.size(-1)
    # (..., seq_len_q, d_k) @ (..., d_k, seq_len_k) -> (..., seq_len_q, seq_len_k)
    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)
    
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
        
    attn_weights = F.softmax(scores, dim=-1)
    
    # (..., seq_len_q, seq_len_k) @ (..., seq_len_v, d_v) -> (..., seq_len_q, d_v)
    output = torch.matmul(attn_weights, v)
    
    return output, attn_weights

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.1):
        """
        Initialize Multi-Head Attention module.
        
        Args:
            d_model (int): Total dimension of the model.
            num_heads (int): Number of attention heads.
            dropout (float, optional): Dropout probability. Defaults to 0.1.
        """
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads # Dimension of each head
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)

    def split_heads(self, x, batch_size):
        """
        Split the last dimension into (num_heads, d_k).
        Transpose to get shape (batch_size, num_heads, seq_len, d_k)
        """
        x = x.view(batch_size, -1, self.num_heads, self.d_k)
        return x.transpose(1, 2)

    def forward(self, q, k, v, mask=None):
        """
        Forward pass for Multi-Head Attention.
        
        Args:
            q (Tensor): Queries. Shape (batch_size, seq_len_q, d_model)
            k (Tensor): Keys. Shape (batch_size, seq_len_k, d_model)
            v (Tensor): Values. Shape (batch_size, seq_len_v, d_model)
            mask (Tensor, optional): Mask. Shape (batch_size, 1, seq_len_q, seq_len_k).
        
        Returns:
            Tensor: Output tensor. Shape (batch_size, seq_len_q, d_model)
        """
        batch_size = q.size(0)

        # 1. Linear projections
        q = self.W_q(q) # (batch_size, seq_len_q, d_model)
        k = self.W_k(k) # (batch_size, seq_len_k, d_model)
        v = self.W_v(v) # (batch_size, seq_len_v, d_model)

        # 2. Split heads
        q = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_q, d_k)
        k = self.split_heads(k, batch_size) # (batch_size, num_heads, seq_len_k, d_k)
        v = self.split_heads(v, batch_size) # (batch_size, num_heads, seq_len_v, d_k)

        # 3. Scaled dot-product attention
        # scaled_attention shape: (batch_size, num_heads, seq_len_q, d_k)
        # attn_weights shape: (batch_size, num_heads, seq_len_q, seq_len_k)
        scaled_attention, attn_weights = scaled_dot_product_attention(q, k, v, mask)

        # 4. Concatenate heads
        # Transpose back: (batch_size, seq_len_q, num_heads, d_k)
        scaled_attention = scaled_attention.transpose(1, 2).contiguous()
        # Combine heads: (batch_size, seq_len_q, d_model)
        concat_attention = scaled_attention.view(batch_size, -1, self.d_model)
        
        # 5. Final linear layer
        output = self.W_o(concat_attention) # (batch_size, seq_len_q, d_model)
        
        return output, attn_weights

# ---------------------------- Feed-Forward Network ----------------------------

class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        """
        Initialize Position-wise Feed-Forward network.
        FFN(x) = max(0, x @ W1 + b1) @ W2 + b2
        
        Args:
            d_model (int): Input and output dimension.
            d_ff (int): Inner dimension.
            dropout (float, optional): Dropout probability. Defaults to 0.1.
        """
        super(PositionwiseFeedForward, self).__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()

    def forward(self, x):
        """
        Forward pass for PFFN.
        
        Args:
            x (Tensor): Input tensor. Shape (batch_size, seq_len, d_model)
            
        Returns:
            Tensor: Output tensor. Shape (batch_size, seq_len, d_model)
        """
        x = self.relu(self.linear1(x))
        x = self.dropout(x)
        x = self.linear2(x)
        return x

# ---------------------------- Positional Encoding ----------------------------

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        """
        Initialize Sinusoidal Positional Encoding.
        
        Args:
            d_model (int): Dimension of the model.
            max_len (int, optional): Maximum sequence length. Defaults to 5000.
            dropout (float, optional): Dropout probability. Defaults to 0.1.
        """
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        # Create positional encoding matrix
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        # Add a batch dimension (1, max_len, d_model)
        pe = pe.unsqueeze(0)
        
        # Register as a buffer, not a parameter
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        Add positional encoding to input tensor.
        
        Args:
            x (Tensor): Input tensor. Shape (batch_size, seq_len, d_model)
            
        Returns:
            Tensor: Output tensor. Shape (batch_size, seq_len, d_model)
        """
        # x is (batch_size, seq_len, d_model)
        # self.pe is (1, max_len, d_model)
        # self.pe[:, :x.size(1), :] is (1, seq_len, d_model)
        x = x + self.pe[:, :x.size(1), :].requires_grad_(False)
        return self.dropout(x)

# ---------------------------- Encoder Layer ----------------------------

class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        """
        Initialize a single Transformer Encoder layer.
        
        Args:
            d_model (int): Model dimension.
            num_heads (int): Number of attention heads.
            d_ff (int): Inner dimension of PFFN.
            dropout (float, optional): Dropout probability. Defaults to 0.1.
        """
        super(EncoderLayer, self).__init__()
        self.mha = MultiHeadAttention(d_model, num_heads, dropout)
        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)

        self.layernorm1 = nn.LayerNorm(d_model, eps=1e-6)
        self.layernorm2 = nn.LayerNorm(d_model, eps=1e-6)
        
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x, mask):
        """
        Forward pass for Encoder layer.
        
        Args:
            x (Tensor): Input tensor. Shape (batch_size, seq_len, d_model)
            mask (Tensor): Source mask. Shape (batch_size, 1, 1, seq_len)
            
        Returns:
            Tensor: Output tensor. Shape (batch_size, seq_len, d_model)
        """
        # 1. Multi-Head Attention (self-attention) + Add & Norm
        attn_output, _ = self.mha(x, x, x, mask)
        x = self.layernorm1(x + self.dropout1(attn_output))
        
        # 2. Feed-Forward + Add & Norm
        ffn_output = self.ffn(x)
        x = self.layernorm2(x + self.dropout2(ffn_output))
        
        return x

# ---------------------------- Decoder Layer ----------------------------

class DecoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        """
        Initialize a single Transformer Decoder layer.
        
        Args:
            d_model (int): Model dimension.
            num_heads (int): Number of attention heads.
            d_ff (int): Inner dimension of PFFN.
            dropout (float, optional): Dropout probability. Defaults to 0.1.
        """
        super(DecoderLayer, self).__init__()
        self.masked_mha = MultiHeadAttention(d_model, num_heads, dropout)
        self.cross_mha = MultiHeadAttention(d_model, num_heads, dropout)
        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)

        self.layernorm1 = nn.LayerNorm(d_model, eps=1e-6)
        self.layernorm2 = nn.LayerNorm(d_model, eps=1e-6)
        self.layernorm3 = nn.LayerNorm(d_model, eps=1e-6)
        
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

    def forward(self, x, enc_output, src_mask, tgt_mask):
        """
        Forward pass for Decoder layer.
        
        Args:
            x (Tensor): Target input. Shape (batch_size, tgt_seq_len, d_model)
            enc_output (Tensor): Encoder output. Shape (batch_size, src_seq_len, d_model)
            src_mask (Tensor): Source mask (for cross-attention). Shape (batch_size, 1, 1, src_seq_len)
            tgt_mask (Tensor): Target mask (for masked self-attention). Shape (batch_size, 1, tgt_seq_len, tgt_seq_len)
            
        Returns:
            Tensor: Output tensor. Shape (batch_size, tgt_seq_len, d_model)
        """
        # 1. Masked Multi-Head Attention (self-attention) + Add & Norm
        attn_output, _ = self.masked_mha(x, x, x, tgt_mask)
        x = self.layernorm1(x + self.dropout1(attn_output))
        
        # 2. Multi-Head Attention (cross-attention) + Add & Norm
        # Q=x (from decoder), K=enc_output, V=enc_output
        cross_attn_output, attn_weights = self.cross_mha(x, enc_output, enc_output, src_mask)
        x = self.layernorm2(x + self.dropout2(cross_attn_output))
        
        # 3. Feed-Forward + Add & Norm
        ffn_output = self.ffn(x)
        x = self.layernorm3(x + self.dropout3(ffn_output))
        
        return x, attn_weights

# ---------------------------- Encoder ----------------------------

class Encoder(nn.Module):
    def __init__(self, num_layers, d_model, num_heads, d_ff, input_vocab_size, max_seq_len, dropout=0.1):
        """
        Initialize the Transformer Encoder.
        
        Args:
            num_layers (int): Number of EncoderLayers.
            d_model (int): Model dimension.
            num_heads (int): Number of attention heads.
            d_ff (int): Inner dimension of PFFN.
            input_vocab_size (int): Size of input vocabulary.
            max_seq_len (int): Maximum sequence length.
            dropout (float, optional): Dropout probability. Defaults to 0.1.
        """
        super(Encoder, self).__init__()
        self.d_model = d_model
        self.embedding = nn.Embedding(input_vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_seq_len, dropout)
        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask):
        """
        Forward pass for Encoder.
        
        Args:
            x (Tensor): Input tensor (indices). Shape (batch_size, seq_len)
            mask (Tensor): Source mask. Shape (batch_size, 1, 1, seq_len)
            
        Returns:
            Tensor: Output tensor. Shape (batch_size, seq_len, d_model)
        """
        seq_len = x.size(1)
        # 1. Embedding + Positional Encoding
        x = self.embedding(x) * math.sqrt(self.d_model) # (batch_size, seq_len, d_model)
        x = self.pos_encoding(x)
        x = self.dropout(x)
        
        # 2. Pass through N EncoderLayers
        for layer in self.layers:
            x = layer(x, mask)
            
        return x

# ---------------------------- Decoder ----------------------------

class Decoder(nn.Module):
    def __init__(self, num_layers, d_model, num_heads, d_ff, target_vocab_size, max_seq_len, dropout=0.1):
        """
        Initialize the Transformer Decoder.
        
        Args:
            num_layers (int): Number of DecoderLayers.
            d_model (int): Model dimension.
            num_heads (int): Number of attention heads.
            d_ff (int): Inner dimension of PFFN.
            target_vocab_size (int): Size of target vocabulary.
            max_seq_len (int): Maximum sequence length.
            dropout (float, optional): Dropout probability. Defaults to 0.1.
        """
        super(Decoder, self).__init__()
        self.d_model = d_model
        self.embedding = nn.Embedding(target_vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, max_seq_len, dropout)
        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, enc_output, src_mask, tgt_mask):
        """
        Forward pass for Decoder.
        
        Args:
            x (Tensor): Target input (indices). Shape (batch_size, tgt_seq_len)
            enc_output (Tensor): Encoder output. Shape (batch_size, src_seq_len, d_model)
            src_mask (Tensor): Source mask. Shape (batch_size, 1, 1, src_seq_len)
            tgt_mask (Tensor): Target mask. Shape (batch_size, 1, tgt_seq_len, tgt_seq_len)
            
        Returns:
            Tensor: Output tensor. Shape (batch_size, tgt_seq_len, d_model)
        """
        seq_len = x.size(1)
        # 1. Embedding + Positional Encoding
        x = self.embedding(x) * math.sqrt(self.d_model)
        x = self.pos_encoding(x)
        x = self.dropout(x)
        
        # 2. Pass through N DecoderLayers
        for layer in self.layers:
            x, attn_weights = layer(x, enc_output, src_mask, tgt_mask)
            
        return x, attn_weights

# ---------------------------- Transformer ----------------------------

class Transformer(nn.Module):
    def __init__(self, num_layers, d_model, num_heads, d_ff, input_vocab_size, target_vocab_size, max_seq_len, dropout=0.1):
        """
        Initialize the Transformer model.
        
        Args:
            num_layers (int): Number of layers (N) for both encoder and decoder.
            d_model (int): Model dimension.
            num_heads (int): Number of attention heads.
            d_ff (int): Inner dimension of PFFN.
            input_vocab_size (int): Source vocabulary size.
            target_vocab_size (int): Target vocabulary size.
            max_seq_len (int): Maximum sequence length.
            dropout (float, optional): Dropout probability. Defaults to 0.1.
        """
        super(Transformer, self).__init__()
        self.encoder = Encoder(num_layers, d_model, num_heads, d_ff, input_vocab_size, max_seq_len, dropout)
        self.decoder = Decoder(num_layers, d_model, num_heads, d_ff, target_vocab_size, max_seq_len, dropout)
        self.final_linear = nn.Linear(d_model, target_vocab_size)

    def create_src_mask(self, src, pad_idx=0):
        """
        Create padding mask for source.
        Shape: (batch_size, 1, 1, src_seq_len)
        """
        src_mask = (src != pad_idx).unsqueeze(1).unsqueeze(2)
        return src_mask

    def create_tgt_mask(self, tgt, pad_idx=0):
        """
        Create padding mask and look-ahead mask for target.
        Shape: (batch_size, 1, tgt_seq_len, tgt_seq_len)
        """
        # 1. Padding mask (batch_size, 1, 1, tgt_seq_len)
        tgt_pad_mask = (tgt != pad_idx).unsqueeze(1).unsqueeze(2)
        
        # 2. Look-ahead mask (1, 1, tgt_seq_len, tgt_seq_len)
        tgt_len = tgt.size(1)
        look_ahead_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=tgt.device)).bool().unsqueeze(0).unsqueeze(0)
        
        # 3. Combine masks
        tgt_mask = tgt_pad_mask & look_ahead_mask
        return tgt_mask

    def forward(self, src, tgt, src_pad_idx=0, tgt_pad_idx=0):
        """
        Forward pass for the Transformer.
        
        Args:
            src (Tensor): Source sequence (indices). Shape (batch_size, src_seq_len)
            tgt (Tensor): Target sequence (indices). Shape (batch_size, tgt_seq_len)
            src_pad_idx (int): Padding index for source.
            tgt_pad_idx (int): Padding index for target.
            
        Returns:
            Tensor: Output logits. Shape (batch_size, tgt_seq_len, target_vocab_size)
        """
        # 1. Create masks
        src_mask = self.create_src_mask(src, src_pad_idx).to(device)
        tgt_mask = self.create_tgt_mask(tgt, tgt_pad_idx).to(device)
        
        # 2. Encoder forward pass
        enc_output = self.encoder(src, src_mask)
        
        # 3. Decoder forward pass
        dec_output, attn_weights = self.decoder(tgt, enc_output, src_mask, tgt_mask)
        
        # 4. Final linear layer
        logits = self.final_linear(dec_output)
        
        return logits

# ---------------------------- Example Usage ----------------------------
# Hyperparameters
NUM_LAYERS = 6
D_MODEL = 512
NUM_HEADS = 8
D_FF = 2048
INPUT_VOCAB_SIZE = 50  # Example vocab size
TARGET_VOCAB_SIZE = 50 # Example vocab size
MAX_SEQ_LEN = 100
DROPOUT = 0.1

model = Transformer(
    num_layers=NUM_LAYERS,
    d_model=D_MODEL,
    num_heads=NUM_HEADS,
    d_ff=D_FF,
    input_vocab_size=INPUT_VOCAB_SIZE,
    target_vocab_size=TARGET_VOCAB_SÄ°ZE,
    max_seq_len=MAX_SEQ_LEN,
    dropout=DROPOUT
).to(device)

# -------------------------- Small forward pass test --------------------------
BATCH = 2
SRC_LEN = 5
TGT_LEN = 6

# Create random input tensors (token indices)
src = torch.randint(1, 50, (BATCH, SRC_LEN), device=device)
tgt = torch.randint(1, 50, (BATCH, TGT_LEN), device=device)

logits = model(src, tgt)
print('Output logits shape:', logits.shape)

# -------------------------- Simple training step --------------------------
criterion = nn.CrossEntropyLoss(ignore_index=0) # Assuming 0 is pad index
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

# Create example target (shift by one)
tgt_in = tgt[:, :-1]  # Input to decoder (e.g., <sos> w1 w2)
tgt_out = tgt[:, 1:]  # Target for loss (e.g., w1 w2 <eos>)

# Forward pass
logits = model(src, tgt_in)

# Compute loss
loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))

# Backward pass and optimization
optimizer.zero_grad()
loss.backward()
optimizer.step()

print('Training step loss:', loss.item())