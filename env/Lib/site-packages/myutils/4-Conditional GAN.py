# Import necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils
from torch.utils.data import DataLoader

# Set hyperparameters and device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

image_size = 64
batch_size = 128
nc = 1           # Number of channels (MNIST is grayscale)
nz = 100         # Size of latent vector z
ngf = 64         # Generator feature map size
ndf = 64         # Discriminator feature map size
num_epochs = 10
lr = 0.0002
beta1 = 0.5
n_classes = 10   # MNIST has 10 classes (0-9)
embedding_dim = 100 # Size of the embedding for the class label

# Data preparation
transform = transforms.Compose([
    transforms.Resize(image_size),
    transforms.CenterCrop(image_size),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,)), # Normalize to [-1, 1]
])

dataset = dset.MNIST(root='./data', download=True, transform=transform)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)

# Weight initialization function (from DCGAN paper)
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)

# Generator Definition
class Generator(nn.Module):
    def __init__(self, nz, ngf, nc, n_classes, embedding_dim):
        super(Generator, self).__init__()
        self.label_embedding = nn.Embedding(n_classes, embedding_dim)
        
        # We combine noise z (nz) and the label embedding (embedding_dim)
        input_dim = nz + embedding_dim
        
        self.main = nn.Sequential(
            # input: combined (nz + embedding_dim) -> (ngf*8) x 4 x 4
            nn.ConvTranspose2d(input_dim, ngf * 8, 4, 1, 0, bias=False),
            nn.BatchNorm2d(ngf * 8),
            nn.ReLU(True),

            # state: (ngf*8) x 4 x 4 -> (ngf*4) x 8 x 8
            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 4),
            nn.ReLU(True),

            # -> (ngf*2) x 16 x 16
            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 2),
            nn.ReLU(True),

            # -> (ngf) x 32 x 32
            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf),
            nn.ReLU(True),

            # -> (nc) x 64 x 64
            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),
            nn.Tanh()  # outputs in [-1, 1]
        )

    def forward(self, noise, labels):
        # Get label embeddings
        label_emb = self.label_embedding(labels) # (batch_size, embedding_dim)
        # Reshape embedding to be concatenated with noise
        label_emb = label_emb.view(label_emb.size(0), self.label_embedding.embedding_dim, 1, 1)
        # Concatenate noise and label embedding
        gen_input = torch.cat((noise, label_emb), 1)
        # Pass through the generator network
        return self.main(gen_input)

# Discriminator Definition
class Discriminator(nn.Module):
    def __init__(self, nc, ndf, n_classes, embedding_dim):
        super(Discriminator, self).__init__()
        self.label_embedding = nn.Embedding(n_classes, embedding_dim)
        
        # We combine image channels (nc) and the label embedding (embedding_dim)
        # We will reshape the embedding and add it as extra channels to the image
        input_channels = nc + embedding_dim
        
        # This part is a bit tricky. We can't just add the embedding as channels
        # easily. A common approach for cGAN is to process the image and
        # embedding separately and combine them later.
        
        # Let's try a simpler approach: reshape embedding to image size and concat
        # This requires knowing the image size beforehand (64x64)
        # New approach: Project label and concatenate with image features
        
        self.label_projection = nn.Sequential(
            nn.Linear(embedding_dim, image_size * image_size),
            nn.ReLU(True)
        )

        self.image_to_features = nn.Sequential(
            # input: (nc) x 64 x 64
            nn.Conv2d(nc, ndf // 2, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
        )
        
        # After concatenating image features and label features
        # The image_to_features output is (ndf/2) x 32 x 32
        # We'll reshape the label embedding to (1) x 32 x 32 after projection
        # Let's rethink. A simpler way is to embed the label and add it as 
        # channels to the input image.

        self.label_embedding = nn.Embedding(n_classes, image_size * image_size)

        # Input is (nc + 1) x 64 x 64
        # We add 1 channel for the reshaped label embedding
        
        self.main = nn.Sequential(
            # input: (nc+1) x 64 x 64 -> (ndf) x 32 x 32
            nn.Conv2d(nc + 1, ndf, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),

            # -> (ndf*2) x 16 x 16
            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 2),
            nn.LeakyReLU(0.2, inplace=True),

            # -> (ndf*4) x 8 x 8
            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 4),
            nn.LeakyReLU(0.2, inplace=True),

            # -> (ndf*8) x 4 x 4
            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 8),
            nn.LeakyReLU(0.2, inplace=True),

            # output: single scalar
            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
        )

    def forward(self, img, labels):
        # Get label embeddings and reshape
        label_emb = self.label_embedding(labels)
        label_emb = label_emb.view(-1, 1, image_size, image_size)
        
        # Concatenate image and label embedding along the channel dimension
        d_input = torch.cat((img, label_emb), 1)
        
        # Pass through discriminator network
        return self.main(d_input).view(-1, 1).squeeze(1)


# Create models
netG = Generator(nz, ngf, nc, n_classes, embedding_dim).to(device)
netD = Discriminator(nc, ndf, n_classes, image_size*image_size).to(device) # Note: updated embedding_dim for D

# Apply weight initialization
netG.apply(weights_init)
netD.apply(weights_init)

# Print models
print(netG)
print(netD)

# Loss and optimizers
criterion = nn.BCELoss()
fixed_noise = torch.randn(64, nz, 1, 1, device=device)

# Create fixed labels for visualization (e.g., 8 of each digit 0-7)
fixed_labels = torch.cat([torch.full((8,), i) for i in range(8)], 0).to(device)

optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))
optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))

# --- Training Loop ---
print("Starting Training...")
img_list = []
G_losses = []
D_losses = []
iters = 0

for epoch in range(num_epochs):
    for i, (data, labels) in enumerate(dataloader, 0):
        
        # ------------------------------------
        # (1) Update D: maximize log(D(x,y)) + log(1 - D(G(z,y), y))
        # ------------------------------------
        netD.zero_grad()
        
        # --- Train with real batch ---
        real_img = data.to(device)
        real_labels = labels.to(device)
        b_size = real_img.size(0)
        
        label_real = torch.full((b_size,), 1., dtype=torch.float, device=device)
        label_fake = torch.full((b_size,), 0., dtype=torch.float, device=device)

        # Forward real batch through D
        output_real = netD(real_img, real_labels)
        errD_real = criterion(output_real, label_real)
        errD_real.backward()
        D_x = output_real.mean().item()

        # --- Train with fake batch ---
        # Generate fake image batch with G
        noise = torch.randn(b_size, nz, 1, 1, device=device)
        # Use the same labels as the real batch for the fake batch
        fake_labels = real_labels 
        
        fake_img = netG(noise, fake_labels)
        
        # Classify all fake batch with D
        output_fake = netD(fake_img.detach(), fake_labels)
        errD_fake = criterion(output_fake, label_fake)
        errD_fake.backward()
        D_G_z1 = output_fake.mean().item()

        # Add the gradients from real and fake batches
        errD = errD_real + errD_fake
        optimizerD.step()

        # ------------------------------------
        # (2) Update G: maximize log(D(G(z,y), y))
        # ------------------------------------
        netG.zero_grad()
        # G wants D to label fakes as real
        output = netD(fake_img, fake_labels) 
        errG = criterion(output, label_real) 
        errG.backward()
        D_G_z2 = output.mean().item()
        optimizerG.step()

        # Output training stats
        if i % 50 == 0:
            print(f'[{epoch+1}/{num_epochs}] [{i}/{len(dataloader)}] '
                  f'Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f} '
                  f'D(x): {D_x:.4f} D(G(z)): {D_G_z1:.4f} / {D_G_z2:.4f}')

        # Save losses for plotting
        G_losses.append(errG.item())
        D_losses.append(errD.item())

        # Check how G is doing by saving G's output on fixed_noise
        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):
            with torch.no_grad():
                fake_fixed = netG(fixed_noise, fixed_labels).detach().cpu()
            img_list.append(vutils.make_grid(fake_fixed, padding=2, normalize=True, nrow=8))
            
            # Save the image grid
            vutils.save_image(img_list[-1], f'./cgan_output/fake_samples_epoch{epoch+1}_iter{iters}.png')


        iters += 1

print("Training finished.")

# Save models
torch.save(netG.state_dict(), './netG_cgan_mnist.pth')
torch.save(netD.state_dict(), './netD_cgan_mnist.pth')