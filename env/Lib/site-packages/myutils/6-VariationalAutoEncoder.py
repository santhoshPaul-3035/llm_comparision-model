import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
from torchvision.datasets import MNIST
from torchvision.utils import save_image, make_grid
import os

# Set hyperparameters
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

num_epochs = 20
batch_size = 128
learning_rate = 0.001
latent_dim = 20  # Latent space dimension
img_dir = './vae_images'
os.makedirs(img_dir, exist_ok=True)

# MNIST dataset
transform = transforms.Compose([
    transforms.ToTensor(),
])

train_dataset = MNIST(root='./data', train=True, transform=transform, download=True)
test_dataset = MNIST(root='./data', train=False, transform=transform, download=True)

train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

# Variational Autoencoder (VAE) Model
class VAE(nn.Module):
    def __init__(self, input_dim=784, h_dim=400, z_dim=20):
        super(VAE, self).__init__()
        
        # Encoder
        self.img_to_hidden = nn.Linear(input_dim, h_dim)
        self.fc_mu = nn.Linear(h_dim, z_dim)        # FC layer for mean (mu)
        self.fc_log_var = nn.Linear(h_dim, z_dim)   # FC layer for log variance (log_var)
        
        # Decoder
        self.hidden_to_img = nn.Linear(z_dim, h_dim)
        self.decode_output = nn.Linear(h_dim, input_dim)
        
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def encode(self, x):
        h = self.relu(self.img_to_hidden(x))
        mu = self.fc_mu(h)
        log_var = self.fc_log_var(h)
        return mu, log_var

    def reparameterize(self, mu, log_var):
        """
        Reparameterization trick: z = mu + std * epsilon
        """
        std = torch.exp(0.5 * log_var)
        epsilon = torch.randn_like(std) # Sample from N(0, 1)
        z = mu + std * epsilon
        return z

    def decode(self, z):
        h = self.relu(self.hidden_to_img(z))
        x_reconstructed = self.sigmoid(self.decode_output(h))
        return x_reconstructed

    def forward(self, x):
        x_flat = x.view(-1, 28 * 28)
        mu, log_var = self.encode(x_flat)
        z = self.reparameterize(mu, log_var)
        x_reconstructed = self.decode(z)
        x_reconstructed_img = x_reconstructed.view(-1, 1, 28, 28)
        
        return x_reconstructed_img, mu, log_var

model = VAE(input_dim=784, h_dim=400, z_dim=latent_dim).to(device)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Loss function (Reconstruction + KL divergence)
def vae_loss_function(x_reconstructed, x, mu, log_var):
    # 1. Reconstruction Loss (BCE)
    # Use BCE loss between original image (x) and reconstructed image
    x_flat = x.view(-1, 784)
    x_reconstructed_flat = x_reconstructed.view(-1, 784)
    BCE = nn.functional.binary_cross_entropy(x_reconstructed_flat, x_flat, reduction='sum')

    # 2. KL Divergence (KLD)
    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())

    return BCE + KLD

# Training loop
print("Starting Training...")
for epoch in range(num_epochs):
    model.train()
    train_loss = 0
    for i, (images, _) in enumerate(train_loader):
        images = images.to(device)
        
        # Forward pass
        reconstructed_img, mu, log_var = model(images)
        
        # Calculate loss
        loss = vae_loss_function(reconstructed_img, images, mu, log_var)
        
        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        train_loss += loss.item()
        optimizer.step()
        
    avg_loss = train_loss / len(train_loader.dataset)
    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')

    # Save reconstructed images and generated samples
    if epoch % 5 == 0 or epoch == num_epochs - 1:
        model.eval()
        with torch.no_grad():
            # 1. Save Reconstructions
            # Get a batch from the test loader
            test_iter = iter(test_loader)
            test_data, _ = next(test_iter)
            test_data = test_data.to(device)
            
            reconstructed, _, _ = model(test_data)
            
            # Combine original and reconstructed
            combined = torch.cat([test_data.cpu(), reconstructed.cpu()])
            grid = make_grid(combined, nrow=batch_size, padding=2, normalize=True)
            save_path = os.path.join(img_dir, f'reconstruction_epoch_{epoch+1}.png')
            save_image(grid, save_path)
            print(f"Saved reconstructed images to {save_path}")

            # 2. Save Generated Samples
            # Sample from the latent space (prior N(0, I))
            z_sample = torch.randn(64, latent_dim).to(device)
            generated_samples = model.decode(z_sample)
            generated_samples_img = generated_samples.view(-1, 1, 28, 28)
            
            grid_gen = make_grid(generated_samples_img.cpu(), nrow=8, padding=2, normalize=True)
            save_path_gen = os.path.join(img_dir, f'generated_epoch_{epoch+1}.png')
            save_image(grid_gen, save_path_gen)
            print(f"Saved generated samples to {save_path_gen}")

print("Training finished.")

# Visualize results
def visualize_vae_output(model, test_loader, device, num_samples=10):
    model.eval()
    with torch.no_grad():
        dataiter = iter(test_loader)
        images, _ = next(dataiter)
        images = images.to(device)
        
        # Get reconstructions
        reconstructions, _, _ = model(images)

        # Move to CPU for plotting
        images = images.cpu()
        reconstructions = reconstructions.cpu()

        # Plot originals vs. reconstructions
        fig, axes = plt.subplots(nrows=2, ncols=num_samples, sharex=True, sharey=True, figsize=(20, 4))
        print("Top row: Original Images")
        print("Bottom row: Reconstructed Images")
        for i in range(num_samples):
            # Original
            ax = axes[0, i]
            ax.imshow(images[i].squeeze(), cmap='gray')
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)
            if i == 0: ax.set_ylabel("Original", rotation=90, size='large')
            
            # Reconstruction
            ax = axes[1, i]
            ax.imshow(reconstructions[i].squeeze(), cmap='gray')
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)
            if i == 0: ax.set_ylabel("Reconstructed", rotation=90, size='large')
        
        plt.suptitle("VAE Reconstructions", fontsize=16)
        plt.show()

        # Plot generated samples
        z_sample = torch.randn(64, latent_dim).to(device)
        generated_samples = model.decode(z_sample).view(-1, 1, 28, 28).cpu()
        
        print("\nGenerated Samples from random noise (z):")
        fig_gen = plt.figure(figsize=(8, 8))
        grid = make_grid(generated_samples, nrow=8, padding=2, normalize=True)
        plt.imshow(grid.permute(1, 2, 0).numpy())
        plt.axis('off')
        plt.suptitle("VAE Generated Samples", fontsize=16)
        plt.show()

visualize_vae_output(model, test_loader, device)