import torch
import torch.nn as nn
import torch.nn.functional as F
import random
import torch.optim as optim
import time
import math

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# -------------------------------
# 1️⃣ Define Attention Mechanism
# (Bahdanau-style attention)
# -------------------------------
class Attention(nn.Module):
    def __init__(self, hidden_dim):
        super(Attention, self).__init__()
        # We need layers to align the encoder hidden state (h) and decoder hidden state (s)
        # We will concatenate them, so hidden_dim * 2
        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)
        # self.v is a vector (or a linear layer with 1 output)
        self.v = nn.Linear(hidden_dim, 1, bias=False)

    def forward(self, hidden, encoder_outputs):
        # hidden shape: (batch_size, hidden_dim)
        # encoder_outputs shape: (batch_size, src_len, hidden_dim)
        
        src_len = encoder_outputs.shape[1]
        
        # Repeat decoder hidden state src_len times
        # hidden shape: (batch_size, src_len, hidden_dim)
        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)
        
        # Concatenate hidden and encoder_outputs
        # energy shape: (batch_size, src_len, hidden_dim * 2)
        energy_input = torch.cat((hidden, encoder_outputs), dim=2)
        
        # Calculate energy (alignment score)
        # energy shape: (batch_size, src_len, hidden_dim)
        energy = torch.tanh(self.attn(energy_input))
        
        # Get attention score (scalar) for each encoder output
        # attention shape: (batch_size, src_len, 1)
        attention = self.v(energy)
        
        # Squeeze to remove the last dimension
        # attention shape: (batch_size, src_len)
        attention = attention.squeeze(2)
        
        # Apply softmax to get weights (sum to 1)
        # a shape: (batch_size, src_len)
        return F.softmax(attention, dim=1)

# -------------------------------
# 2️⃣ Define Encoder
# -------------------------------
class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hidden_dim, dropout):
        super().__init__()
        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.rnn = nn.GRU(emb_dim, hidden_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, src):
        # src shape: (src_len, batch_size)
        
        # embedded shape: (src_len, batch_size, emb_dim)
        embedded = self.dropout(self.embedding(src))
        
        # outputs shape: (src_len, batch_size, hidden_dim)
        # hidden shape: (1, batch_size, hidden_dim)
        outputs, hidden = self.rnn(embedded)
        
        # outputs are all hidden states, hidden is the last hidden state
        return outputs, hidden

# -------------------------------
# 3️⃣ Define Decoder (with Attention)
# -------------------------------
class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hidden_dim, dropout, attention):
        super().__init__()
        self.output_dim = output_dim
        self.attention = attention
        
        self.embedding = nn.Embedding(output_dim, emb_dim)
        # Input to GRU is [embedding + context vector]
        self.rnn = nn.GRU(emb_dim + hidden_dim, hidden_dim)
        self.fc_out = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, input, hidden, encoder_outputs):
        # input shape: (batch_size) -> [e.g., <sos> token]
        # hidden shape: (1, batch_size, hidden_dim) -> [decoder hidden state]
        # encoder_outputs shape: (src_len, batch_size, hidden_dim) -> [all encoder hidden states]

        # Add sequence length dimension to input
        # input shape: (1, batch_size)
        input = input.unsqueeze(0)
        
        # embedded shape: (1, batch_size, emb_dim)
        embedded = self.dropout(self.embedding(input))
        
        # Get attention weights
        # We need to permute encoder_outputs for batch_first processing in Attention
        # encoder_outputs permuted: (batch_size, src_len, hidden_dim)
        # hidden permuted: (batch_size, hidden_dim)
        a = self.attention(hidden.squeeze(0), encoder_outputs.permute(1, 0, 2))
        
        # a shape: (batch_size, src_len)
        # Add unsqueeze for bmm
        # a shape: (batch_size, 1, src_len)
        a = a.unsqueeze(1)
        
        # encoder_outputs permuted: (batch_size, src_len, hidden_dim)
        
        # Calculate context vector (weighted sum)
        # c shape: (batch_size, 1, hidden_dim)
        c = torch.bmm(a, encoder_outputs.permute(1, 0, 2))
        
        # Permute c back to (1, batch_size, hidden_dim)
        c = c.permute(1, 0, 2)
        
        # Concatenate embedding and context vector
        # rnn_input shape: (1, batch_size, emb_dim + hidden_dim)
        rnn_input = torch.cat((embedded, c), dim=2)
        
        # Pass through GRU
        # output shape: (1, batch_size, hidden_dim)
        # hidden shape: (1, batch_size, hidden_dim)
        output, hidden = self.rnn(rnn_input, hidden)
        
        # Pass through final linear layer to get prediction
        # prediction shape: (batch_size, output_dim)
        prediction = self.fc_out(output.squeeze(0))
        
        return prediction, hidden

# -------------------------------
# 4️⃣ Define Seq2Seq Model
# -------------------------------
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        # src shape: (src_len, batch_size)
        # trg shape: (trg_len, batch_size)
        
        batch_size = trg.shape[1]
        trg_len = trg.shape[0]
        trg_vocab_size = self.decoder.output_dim
        
        # Tensor to store decoder outputs
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)
        
        # Encode source sequence
        encoder_outputs, hidden = self.encoder(src)
        
        # First input to decoder is the <sos> token
        input = trg[0, :]
        
        for t in range(1, trg_len):
            # Run one step of decoder
            output, hidden = self.decoder(input, hidden, encoder_outputs)
            
            # Store output
            outputs[t] = output
            
            # Decide whether to use teacher forcing
            teacher_force = random.random() < teacher_forcing_ratio
            
            # Get the highest predicted token
            top1 = output.argmax(1)
            
            # If teacher forcing, use actual next token
            # Otherwise, use predicted token
            input = trg[t] if teacher_force else top1
            
        return outputs

# -------------------------------
# 5️⃣ Prepare Data (Toy Example)
# (Reversing a sequence of words)
# -------------------------------
# This is a very simple "vocabulary"
data = [
    ("hello world", "world hello"),
    ("how are you", "you are how"),
    ("i am fine", "fine am i"),
    ("good morning", "morning good"),
    ("good night", "night good"),
    ("see you later", "later you see"),
    ("nice to meet you", "you meet to nice"),
    ("what is your name", "name your is what")
]

# Build vocabulary
words = set()
for en, fr in data:
    words.update(en.split())
    words.update(fr.split())

words = ["<pad>", "<sos>", "<eos>"] + sorted(list(words))
stoi = {s: i for i, s in enumerate(words)}
itos = {i: s for i, s in enumerate(words)}

def tokenize(text):
    return [s for s in text.split()]

def encode_seq(text):
    return [stoi[s] for s in tokenize(text)]

def decode_seq(indices):
    return ' '.join([itos[i] for i in indices])

# Create tensors
X = [torch.tensor(encode_seq(en), dtype=torch.long) for en, _ in data]
Y = [torch.tensor(encode_seq(fr), dtype=torch.long) for _, fr in data]

# Add <sos> and <eos>
X = [torch.cat([torch.tensor([stoi["<sos>"]]), x, torch.tensor([stoi["<eos>"]])]) for x in X]
Y = [torch.cat([torch.tensor([stoi["<sos>"]]), y, torch.tensor([stoi["<eos>"]])]) for y in Y]

# Pad sequences (simple padding to max length in this batch)
max_x_len = max(len(x) for x in X)
max_y_len = max(len(y) for y in Y)

X_pad = [F.pad(x, (0, max_x_len - len(x)), value=stoi["<pad>"]) for x in X]
Y_pad = [F.pad(y, (0, max_y_len - len(y)), value=stoi["<pad>"]) for y in Y]

# Stack into a batch
# We need (seq_len, batch_size)
X_batch = torch.stack(X_pad, dim=1).to(device)
Y_batch = torch.stack(Y_pad, dim=1).to(device)

print("Source batch shape:", X_batch.shape) # (src_len, batch_size)
print("Target batch shape:", Y_batch.shape) # (trg_len, batch_size)

# -------------------------------
# 6️⃣ Initialize Model & Training
# -------------------------------

# Hyperparameters
INPUT_DIM = len(stoi)
OUTPUT_DIM = len(stoi)
EMB_DIM = 64
HIDDEN_DIM = 128
DROPOUT = 0.1
N_EPOCHS = 500

# Initialize modules
attn = Attention(HIDDEN_DIM).to(device)
enc = Encoder(INPUT_DIM, EMB_DIM, HIDDEN_DIM, DROPOUT).to(device)
dec = Decoder(OUTPUT_DIM, EMB_DIM, HIDDEN_DIM, DROPOUT, attn).to(device)

model = Seq2Seq(enc, dec, device).to(device)

# Initialize weights
def init_weights(m):
    for name, param in m.named_parameters():
        if 'weight' in name:
            nn.init.normal_(param.data, mean=0, std=0.01)
        else:
            nn.init.constant_(param.data, 0)
            
model.apply(init_weights)

optimizer = optim.Adam(model.parameters())
criterion = nn.CrossEntropyLoss(ignore_index=stoi["<pad>"])

def train(model, X_batch, Y_batch, optimizer, criterion):
    model.train()
    optimizer.zero_grad()
    
    # Y_batch shape: (trg_len, batch_size)
    # X_batch shape: (src_len, batch_size)
    output = model(X_batch, Y_batch, teacher_forcing_ratio=0.7)
    
    # output shape: (trg_len, batch_size, output_dim)
    # Y_batch shape: (trg_len, batch_size)
    
    # Reshape for loss calculation
    # output_dim = (trg_len * batch_size, output_dim)
    # target_dim = (trg_len * batch_size)
    
    # Skip <sos> token in loss
    output_flat = output[1:].view(-1, output.shape[-1])
    target_flat = Y_batch[1:].view(-1)
    
    loss = criterion(output_flat, target_flat)
    loss.backward()
    optimizer.step()
    
    return loss.item()

print("\nStarting training...")
for epoch in range(N_EPOCHS):
    loss = train(model, X_batch, Y_batch, optimizer, criterion)
    if (epoch + 1) % 50 == 0:
        print(f'Epoch: {epoch+1:03} | Loss: {loss:.3f}')

print("Training finished.")

# -------------------------------
# 7️⃣ Prediction/Inference
# -------------------------------
def predict(input_text, max_len=10):
    model.eval()
    with torch.no_grad():
        # Encode input
        tokens = [stoi["<sos>"]] + encode_seq(input_text) + [stoi["<eos>"]]
        src = torch.tensor(tokens, dtype=torch.long).unsqueeze(1).to(device) # (src_len, 1)

        encoder_outputs, hidden = model.encoder(src)
        
        # Start decoding
        input = torch.tensor([stoi["<sos>"]], device=device)
        generated = []
        
        for _ in range(max_len):
            output, hidden = model.decoder(input, hidden, encoder_outputs)
            top1 = output.argmax(1)
            
            if top1.item() == stoi["<eos>"]:
                break
            
            generated.append(top1.item())
            input = top1 # Use prediction as next input
            
        return decode_seq(generated)

# -------------------------------
# 8️⃣ Try It Out
# -------------------------------
while True:
    inp = input("\nEnter a word (or 'quit'): ").lower().strip()
    if inp == "quit":
        break
    if inp not in [d[0] for d in data]:
        print("Word not in training data, but let's try...")
    print("Generated output:", predict(inp))