import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# ----------------------------
# Example dataset (toy text)
# ----------------------------
text = "hello welcome to the RNN sequence generation program. this is a simple example to show how character-level RNNs work."
chars = sorted(list(set(text)))
vocab_size = len(chars)

# Char â†” Index mapping
char_to_idx = {ch: i for i, ch in enumerate(chars)}
idx_to_char = {i: ch for i, ch in enumerate(chars)}

# ----------------------------
# Prepare data (sequences)
# ----------------------------
seq_length = 10
dataX = []
dataY = []

for i in range(0, len(text) - seq_length):
    seq_in = text[i : i + seq_length]
    seq_out = text[i + 1 : i + seq_length + 1]
    dataX.append([char_to_idx[char] for char in seq_in])
    dataY.append([char_to_idx[char] for char in seq_out])

n_patterns = len(dataX)
print("Total patterns:", n_patterns)

# Reshape X to be [samples, seq_length, 1] (or [samples, seq_length] for Embedding)
# Convert to tensors
X = torch.tensor(dataX, dtype=torch.long)
Y = torch.tensor(dataY, dtype=torch.long)

# ----------------------------
# RNN Model (using Embedding layer)
# ----------------------------
class CharRNN(nn.Module):
    def __init__(self, vocab_size, hidden_size, embedding_size, n_layers=1):
        super(CharRNN, self).__init__()
        self.hidden_size = hidden_size
        self.n_layers = n_layers
        
        self.embedding = nn.Embedding(vocab_size, embedding_size)
        self.rnn = nn.RNN(embedding_size, hidden_size, n_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, x, hidden):
        # x shape: (batch_size, seq_length)
        embedded = self.embedding(x)
        # embedded shape: (batch_size, seq_length, embedding_size)
        
        # rnn_out shape: (batch_size, seq_length, hidden_size)
        # hidden shape: (n_layers, batch_size, hidden_size)
        rnn_out, hidden = self.rnn(embedded, hidden)
        
        # Reshape output for the fully connected layer
        # (batch_size * seq_length, hidden_size)
        rnn_out_flat = rnn_out.reshape(-1, self.hidden_size)
        
        # output shape: (batch_size * seq_length, vocab_size)
        output = self.fc(rnn_out_flat)
        
        return output, hidden

    def init_hidden(self, batch_size):
        # Initialize hidden state (n_layers, batch_size, hidden_size)
        return torch.zeros(self.n_layers, batch_size, self.hidden_size)

# Hyperparameters
hidden_size = 128
embedding_size = 64
n_layers = 1
lr = 0.001
num_epochs = 100

model = CharRNN(vocab_size, hidden_size, embedding_size, n_layers)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=lr)

# ----------------------------
# Training
# ----------------------------
print("Starting training...")
for epoch in range(num_epochs):
    # We use the full data as one batch
    hidden = model.init_hidden(1) # Re-initialize hidden state
    
    # X_batch shape: (1, seq_length)
    # Y_batch shape: (1, seq_length)
    # Note: This is inefficient. A real implementation uses DataLoader and batches.
    # For this simple example, we train on one pattern at a time, 
    # but let's train on the whole sequence batch.
    
    # X shape: (n_patterns, seq_length)
    # Y shape: (n_patterns, seq_length)
    
    # Let's adjust the training to be simpler: one long sequence
    X_input = torch.tensor([char_to_idx[c] for c in text[:-1]], dtype=torch.long).unsqueeze(0)
    Y_target = torch.tensor([char_to_idx[c] for c in text[1:]], dtype=torch.long)

    model.zero_grad()
    hidden = model.init_hidden(1)
    
    output, hidden = model(X_input, hidden)
    
    # output shape: (1 * (len(text)-1), vocab_size)
    # Y_target shape: (len(text)-1)
    loss = criterion(output, Y_target)
    
    loss.backward()
    optimizer.step()
    
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

print("Training finished.")

# ----------------------------
# Sequence Generation
# ----------------------------
def generate_sequence(model, start_str, length=100):
    model.eval()
    with torch.no_grad():
        # Initialize hidden state
        hidden = model.init_hidden(1)
        
        # Prepare starting input
        start_input = torch.tensor([[char_to_idx[c] for c in start_str]], dtype=torch.long)
        
        # Feed the starting string to the model
        output, hidden = model(start_input, hidden)
        
        # Get the last character's prediction
        last_char_idx = torch.argmax(output[-1]).item()
        
        result = list(start_str)
        result.append(idx_to_char[last_char_idx])

        # Generate remaining characters
        for _ in range(length - len(start_str)):
            input_eval = torch.tensor([[last_char_idx]], dtype=torch.long)
            output, hidden = model(input_eval, hidden)
            
            # Get the character with the highest probability
            # last_char_idx = torch.argmax(output.squeeze()).item()
            
            # Alternative: Sample from the distribution
            prob = torch.softmax(output.squeeze(), dim=0)
            last_char_idx = torch.multinomial(prob, 1).item()
            
            result.append(idx_to_char[last_char_idx])
            
        return ''.join(result)

# Generate text
print("\nGenerated text (Greedy):")
print(generate_sequence(model, start_str="hello", length=150))