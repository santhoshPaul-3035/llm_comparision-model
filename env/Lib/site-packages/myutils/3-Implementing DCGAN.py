# dcgan_cifar10.py
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, utils
from torch.utils.data import DataLoader

# --------- Hyperparameters ----------
data_root = './data'
out_dir = './dcgan_output'
os.makedirs(out_dir, exist_ok=True)

batch_size = 128
image_size = 64
nc = 3            # number of channels (3 for RGB)
nz = 100          # size of latent vector (noise)
ngf = 64          # generator feature map size
ndf = 64          # discriminator feature map size
num_epochs = 25
lr = 0.0002
beta1 = 0.5       # adam beta1
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# --------- Data transforms & loader ----------
transform = transforms.Compose([
    transforms.Resize(image_size),
    transforms.CenterCrop(image_size),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*nc, [0.5]*nc)   # map to [-1,1]
])

dataset = datasets.CIFAR10(root=data_root, download=True, transform=transform)
loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)

# --------- Weight initialization (DCGAN paper) ----------
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)

# --------- Generator (DCGAN) ----------
class Generator(nn.Module):
    def __init__(self, nz, ngf, nc):
        super(Generator, self).__init__()
        # Input is Z, going into a convolution
        self.main = nn.Sequential(
            # input Z: (nz) -> (ngf*8) x 4 x 4
            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),
            nn.BatchNorm2d(ngf * 8),
            nn.ReLU(True),

            # state: (ngf*8) x 4 x 4 -> (ngf*4) x 8 x 8
            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 4),
            nn.ReLU(True),

            # -> (ngf*2) x 16 x 16
            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 2),
            nn.ReLU(True),

            # -> (ngf) x 32 x 32
            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf),
            nn.ReLU(True),

            # -> (nc) x 64 x 64
            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),
            nn.Tanh()  # outputs in [-1, 1]
        )

    def forward(self, input):
        return self.main(input)

# --------- Discriminator (DCGAN) ----------
class Discriminator(nn.Module):
    def __init__(self, nc, ndf):
        super(Discriminator, self).__init__()
        self.main = nn.Sequential(
            # input: (nc) x 64 x 64 -> (ndf) x 32 x 32
            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),

            # -> (ndf*2) x 16 x 16
            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 2),
            nn.LeakyReLU(0.2, inplace=True),

            # -> (ndf*4) x 8 x 8
            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 4),
            nn.LeakyReLU(0.2, inplace=True),

            # -> (ndf*8) x 4 x 4
            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 8),
            nn.LeakyReLU(0.2, inplace=True),

            # output: single scalar
            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()  # probability real/fake
        )

    def forward(self, input):
        return self.main(input).view(-1, 1).squeeze(1)

# --------- Create models ----------
netG = Generator(nz, ngf, nc).to(device)
netD = Discriminator(nc, ndf).to(device)

netG.apply(weights_init)
netD.apply(weights_init)

# --------- Loss and optimizers ----------
criterion = nn.BCELoss()
fixed_noise = torch.randn(64, nz, 1, 1, device=device)  # for monitoring progress

optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))
optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))

# --------- Training loop ----------
print("Starting Training...")
iters = 0
for epoch in range(num_epochs):
    for i, (data, _) in enumerate(loader):
        # ------------------------------------
        # (1) Update D: maximize log(D(x)) + log(1 - D(G(z)))
        # ------------------------------------
        netD.zero_grad()
        real_cpu = data.to(device)
        b_size = real_cpu.size(0)
        label_real = torch.full((b_size,), 1., dtype=torch.float, device=device)
        label_fake = torch.full((b_size,), 0., dtype=torch.float, device=device)

        # Forward real batch through D
        output_real = netD(real_cpu)
        errD_real = criterion(output_real, label_real)
        errD_real.backward()
        D_x = output_real.mean().item()

        # Generate fake image batch with G
        noise = torch.randn(b_size, nz, 1, 1, device=device)
        fake = netG(noise)
        # Classify all fake batch with D
        output_fake = netD(fake.detach())
        errD_fake = criterion(output_fake, label_fake)
        errD_fake.backward()
        D_G_z1 = output_fake.mean().item()

        # Add the gradients from real and fake batches
        errD = errD_real + errD_fake
        optimizerD.step()

        # ------------------------------------
        # (2) Update G: maximize log(D(G(z)))  <-> minimize BCE(D(G(z)), 1)
        # ------------------------------------
        netG.zero_grad()
        output = netD(fake)  # note: don't detach â€” we want gradients to flow to G
        errG = criterion(output, label_real)  # generator wants D to label fakes as real
        errG.backward()
        D_G_z2 = output.mean().item()
        optimizerG.step()

        if i % 100 == 0:
            print(f"Epoch [{epoch+1}/{num_epochs}] Iter [{i}/{len(loader)}] "
                  f"Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f} "
                  f"D(x): {D_x:.4f} D(G(z)): {D_G_z1:.4f}/{D_G_z2:.4f}")

        # ---- Save images every few iterations ----
        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(loader)-1)):
            with torch.no_grad():
                fake_fixed = netG(fixed_noise).detach().cpu()
            grid = utils.make_grid(fake_fixed, padding=2, normalize=True)
            utils.save_image(grid, os.path.join(out_dir, f'fake_samples_epoch{epoch+1}_iter{iters}.png'))

        iters += 1

    # Save checkpoints per epoch
    torch.save(netG.state_dict(), os.path.join(out_dir, f"netG_epoch_{epoch+1}.pth"))
    torch.save(netD.state_dict(), os.path.join(out_dir, f"netD_epoch_{epoch+1}.pth"))

print("Training finished.")